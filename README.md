# :secret:AI产品可能会问到的面试题-鹤宁:cancer:专用版

题目的难度从简单到复杂:arrow_double_down:，后面的题如果觉得有点难理解就别看了



+ 什么样的场景可以用模型

  + 重复的工作

  + 人不需要复杂的思考就能完成的

  + 有规律可循的

    

+ 模型开发流程

  + 技术和产品确认可行性

  + 数据标注，技术可以采用一些小样本学习的方法来提前做一些伪标签的标注，人再来二次校验

  + 模型更新迭代，上线后，把线上的数据再继续人工标注，然后反哺给模型训练，这个过程也叫作active learning(总有人能取一些奇奇怪怪的名字)

    

+ 工业场景其实很缺数据，怎么做数据扩充:construction:

  + 图像如何做数据增强

    旋转，裁剪，平移，加高斯噪音

  + 文本如何数据增强

    替换同意词，随机删词，添加词，回译(先翻译成英文，再翻译回来)，还有一种叫做对抗训练的东西，如果碰到了可以简单提一嘴，其它增强方法涉及到技术了，我就不说了

    

+ 监督学习和非监督学习的区别

  监督学习需要有标签，例如有效问模型，非监督模型通常不需要人工标注，例如判断两个句子是不是前后文关系，聚类也是非监督的，聚类指的是把数据自动分成N个类别，例如根据用户兴趣划分不同的组(不同的组就能推荐不同的商品了)

  

+ 模型上线后线上效果不理想的原因

  线上数据和训练数据分布不一致，业务的改动，新产品的上线，可能会有很多不同分布的新数据，所以需要持续的迭代更新，或者说开发从模型的角度尽可能提升模型的泛化能力

  

+ 准确率、精确率、召回率、F1

  这里主要是说一下数据不均衡的场景不能用准确率，例如100条数据，99条正样本，1条负样本，全部分成正例，那准确率是99%，但是可能这个模型完全不具备区分负例的能力(之前py那边就有人用这招忽悠不懂算法的leader)，这个时候就需要用到精确率、召回率和F1了，F1是同时考虑了precision和recall的，一般precision想提高recall可能会降低。

  

+ 什么是迁移学习，什么预训练模型

  + 迁移学习是指在某个场景训练好了一个模型，例如猫狗识别，那就把这个训练好的模型的权重应用到海洋生物识别的场景上训练，这样能很快收敛
  + 预训练模型是指在大规模的数据上做一些非监督或监督训练(主要是非监督)，例如很多文本数据，我们可以可以把一些字词遮住，让模型学习做完形填空。这样训练好的模型实际上已经学习到了很多通用文本特证。接下来就可以直接把这个模型应用到分类任务上再继续训练下，收敛会很快效果也很好，目前nlp主流的方案都是用预训练语言模型(有效问模型也是这么做的hhh)。

  

+ CV有哪些应用场景

  + 图像识别(分辨猫狗)；
  + 目标检测，就是视频里有个框一直在锁定这人脸之类的，例如下图，右边这种目标检测做的更好。深圳交警搞了个摄像头检测司机和副驾驶有没有系安全带；

  ![image-20210823225041976](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hZTAxLmFsaWNkbi5jb20va2YvVVRCOFhaTXF2cFBKWEtKa1NhaFZxNnh5ekZYYTAuanBn)

  + 图像还有一块就是生成假图了，听说淘宝双11好多商品宣传图都是自动生成的；

  + 其它就是什么换脸去码啥的:underage:。

    

+ NLP有哪些应用场景

  + 文本分类；

  + 文本匹配(判断两句话是否是同义句，智能问答就是这么做的，看下FAQ库里哪个问题的相似度最高，然后返回答案，一般算余弦相似度，因为值域在[-1,1]之间好取个阈值，假如阈值是0.8，那么低于0.8的情况就认为没有合适的答案)

  + 文本生成：摘要，对对联，闲聊，续写文章

  + 实体识别，找到一句话中的关键词，例如政府机构，地名，人名

    ```
    e.g.最近陈奕迅发了一首叫做《不期而遇的夏天》的歌
    这个句子中一个人名实体：陈奕迅，还有个歌名实体：《不期而遇的夏天》
    ```

  + 其他还有一些不常见的任务，例如错别字检测，阅读理解，完形填空等，这些你应该不用了解

  

+ 常见的深度学习模型

  + CNN 卷积神经网络，主要用于处理图像，也可以处理文本，但是只能处理短文本

  + RNN 循环神经网络，主要用于处理序列数据(序列数据值的是例如文本，音频，视频，这样串行的或者说和时间有关联的)，但是RNN效果不太行，所以也有RNN的改进版本，叫做LSTM

  + Transformer 目前很厉害的模型，也可以用来处理序列数据(例如之前说的音频识别)，年初有一篇论文出了个模型叫ViT基础结构就是Transformer ，NLP中目前热门的模型都是Transformer 结构，例如bert(主要用来做自然语言理解的，例如分类，文本匹配等)，gpt(主要用来做文本生成的)

    

**就写这么多吧，再多估计你也没空看，面试加油哦~**:beers:
